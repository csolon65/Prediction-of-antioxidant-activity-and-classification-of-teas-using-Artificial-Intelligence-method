# -*- coding: utf-8 -*-
"""Classification CSMF.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mWymsPVwG8NR2yxuTiawy3umrsmBNEAd
"""

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from PIL import Image
import tensorflow as tf
from sklearn.utils import shuffle
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, LSTM, GlobalAveragePooling1D
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix
import pandas as pd
from tensorflow.keras.layers import Flatten, Dense
from sklearn.preprocessing import StandardScaler
from tensorflow.keras.utils import to_categorical

# Montage du Google Drive
from google.colab import drive
drive.mount('/content/drive')
directory_path = "/content/drive/My Drive/"

# Chargement des fichiers csv depuis le drive

vert_2301_names = []
for i in range(1, 30):
    vert_2301_names.append(f"V{i} 230 1")

# Afficher la liste complète
print(vert_2301_names)

vert_2302_names = []
for i in range(1, 30):
    vert_2302_names.append(f"V{i} 230 2")

# Afficher la liste complète
print(vert_2302_names)

vert_517_names = []
for i in range(1, 30):
    vert_517_names.append(f"V{i} 517")

# Afficher la liste complète
print(vert_517_names)

# Chargement des fichiers csv depuis le drive

noir_2301_names = []
for i in range(1, 22):
    noir_2301_names.append(f"N{i} 230 1")

# Afficher la liste complète
print(noir_2301_names)

noir_2302_names = []
for i in range(1, 22):
    noir_2302_names.append(f"N{i} 230 2")

# Afficher la liste complète
print(noir_2302_names)

noir_517_names = []
for i in range(1, 22):
    noir_517_names.append(f"N{i} 517")

# Afficher la liste complète
print(noir_517_names)

# Chargement des fichiers csv depuis le drive

blanc_2301_names = []
for i in range(1, 11):
    blanc_2301_names.append(f"B{i} 230 1")

# Afficher la liste complète
print(blanc_2301_names)

blanc_2302_names = []
for i in range(1,11):
    blanc_2302_names.append(f"B{i} 230 2")

# Afficher la liste complète
print(blanc_2302_names)

blanc_517_names = []
for i in range(1,11):
    blanc_517_names.append(f"B{i} 517")

# Afficher la liste complète
print(blanc_517_names)

# Dictionnaire pour stocker les DataFrames de chaque fichier CSV
vert_2301 = {}

# Importer chaque fichier CSV dans un DataFrame et stocker dans le dictionnaire
for nom_fichier in vert_2301_names:
    vert_2301[nom_fichier] = pd.read_csv("/content/drive/My Drive/vert/" + nom_fichier + ".CSV", delimiter = ';', index_col = 0, header = None)

# Dictionnaire pour stocker les DataFrames de chaque fichier CSV
vert_2302 = {}

# Importer chaque fichier CSV dans un DataFrame et stocker dans le dictionnaire
for nom_fichier in vert_2302_names:
    vert_2302[nom_fichier] = pd.read_csv("/content/drive/My Drive/vert/" + nom_fichier + ".CSV", delimiter = ';', index_col = 0, header = None)

# Dictionnaire pour stocker les DataFrames de chaque fichier CSV
vert_517 = {}

# Importer chaque fichier CSV dans un DataFrame et stocker dans le dictionnaire
for nom_fichier in vert_517_names:
    vert_517[nom_fichier] = pd.read_csv("/content/drive/My Drive/vert/" + nom_fichier + ".CSV", delimiter = ';', index_col = 0, header = None)

# Dictionnaire pour stocker les DataFrames de chaque fichier CSV
noir_2301 = {}

# Importer chaque fichier CSV dans un DataFrame et stocker dans le dictionnaire
for nom_fichier in noir_2301_names:
    noir_2301[nom_fichier] = pd.read_csv("/content/drive/My Drive/noir/" + nom_fichier + ".CSV", delimiter = ';', index_col = 0, header = None)

# Dictionnaire pour stocker les DataFrames de chaque fichier CSV
noir_2302 = {}

# Importer chaque fichier CSV dans un DataFrame et stocker dans le dictionnaire
for nom_fichier in noir_2302_names:
    noir_2302[nom_fichier] = pd.read_csv("/content/drive/My Drive/noir/" + nom_fichier + ".CSV", delimiter = ';', index_col = 0, header = None)

# Dictionnaire pour stocker les DataFrames de chaque fichier CSV
noir_517 = {}

# Importer chaque fichier CSV dans un DataFrame et stocker dans le dictionnaire
for nom_fichier in noir_517_names:
    noir_517[nom_fichier] = pd.read_csv("/content/drive/My Drive/noir/" + nom_fichier + ".CSV", delimiter = ';', index_col = 0, header = None)

# Dictionnaire pour stocker les DataFrames de chaque fichier CSV
blanc_2301 = {}

# Importer chaque fichier CSV dans un DataFrame et stocker dans le dictionnaire
for nom_fichier in blanc_2301_names:
    blanc_2301[nom_fichier] = pd.read_csv("/content/drive/My Drive/blanc/" + nom_fichier + ".CSV", delimiter = ';', index_col = 0, header = None)

# Dictionnaire pour stocker les DataFrames de chaque fichier CSV
blanc_2302 = {}

# Importer chaque fichier CSV dans un DataFrame et stocker dans le dictionnaire
for nom_fichier in blanc_2302_names:
    blanc_2302[nom_fichier] = pd.read_csv("/content/drive/My Drive/blanc/" + nom_fichier + ".CSV", delimiter = ';', index_col = 0, header = None)

# Dictionnaire pour stocker les DataFrames de chaque fichier CSV
blanc_517 = {}

# Importer chaque fichier CSV dans un DataFrame et stocker dans le dictionnaire
for nom_fichier in blanc_517_names:
    blanc_517[nom_fichier] = pd.read_csv("/content/drive/My Drive/blanc/" + nom_fichier + ".CSV", delimiter = ';', index_col = 0, header = None)

# Créer un nouveau dictionnaire pour stocker la fusion des trois dictionnaires
vert = {}

# Concaténer les trois dictionnaires
vert.update(vert_2301)
vert.update(vert_2302)
vert.update(vert_517)

# Créer un nouveau dictionnaire pour stocker la fusion des trois dictionnaires
noir = {}

# Concaténer les trois dictionnaires
noir.update(noir_2301)
noir.update(noir_2302)
noir.update(noir_517)

# Créer un nouveau dictionnaire pour stocker la fusion des trois dictionnaires
blanc = {}

# Concaténer les trois dictionnaires
blanc.update(blanc_2301)
blanc.update(blanc_2302)
blanc.update(blanc_517)

the = {}

the.update(vert)
the.update(noir)
the.update(blanc)

the230 = {}

the230.update(vert_2301)
the230.update(noir_2301)
the230.update(blanc_2301)


# Créer un nouveau dictionnaire pour stocker les DataFrames tronqués
truncated_dataframes = {}

# Parcourir chaque DataFrame dans le dictionnaire
for key, df in the230.items():
    # Tronquer le DataFrame à 5250 lignes
    truncated_dataframes[key] = df.head(5250)

# Remplacer le dictionnaire original par le dictionnaire contenant les DataFrames tronqués
the230 = truncated_dataframes

data_with_labels = []

# Label encoding for the tea types
label_encoder = LabelEncoder()
labels = ['vert', 'noir', 'blanc']
encoded_labels = label_encoder.fit_transform(labels)

# Iterate through the dictionary to process and label the data
for key, df in the230.items():
    # Ensure all values are floats
    df = df.applymap(lambda x: float(str(x).replace(',', '.')))
    # Assign labels based on the key
    if key in vert_2301:
        label = 'vert'
    elif key in noir_2301:
        label = 'noir'
    elif key in blanc_2301:
        label = 'blanc'
    # Append the dataframe and its encoded label to the list
    data_with_labels.append((df, label_encoder.transform([label])[0]))

# Shuffle the data to ensure a good mix
data_with_labels = shuffle(data_with_labels)


# Separate the data and labels
X_data = [data[0].values for data in data_with_labels]  # Convert DataFrame to numpy array
y_labels = [data[1] for data in data_with_labels]

# Normalize the data
scaler = StandardScaler()
X_data_normalized = [scaler.fit_transform(x) for x in X_data]

# Split the dataset into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X_data_normalized, y_labels, test_size=0.4, random_state=42)

# Convert lists to NumPy arrays
X_train = np.array(X_train, dtype=object)
X_test = np.array(X_test, dtype=object)
y_train = np.array(y_train)
y_test = np.array(y_test)

# Ensure all input arrays have the same shape
# This step is crucial and needs to be adjusted based on the actual shape of your data
# For demonstration, let's assume all sequences are padded to have the same length

X_train_padded = pad_sequences(X_train, dtype='float32', padding='post')
X_test_padded = pad_sequences(X_test, dtype='float32', padding='post')

model = Sequential([
    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),
    Dense(64, activation='relu'),
    Dense(64, activation='relu'),
    Dense(3, activation='softmax')
])


# Compile the model
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Train the model
model.fit(X_train_padded, y_train, epochs=20, batch_size=32, validation_split=0.2)

# Evaluate the model
test_loss, test_acc = model.evaluate(X_test_padded, y_test)
print('Test accuracy:', test_acc)

# Faire des prédictions sur l'ensemble de test
y_pred = model.predict(X_test_padded)

# Convertir les prédictions en classes prédites (indices)
y_pred_classes = np.argmax(y_pred, axis=1)

# Calculer la matrice de confusion
conf_matrix = confusion_matrix(y_test, y_pred_classes)

# Définir l'ordre des étiquettes à afficher
label_order = ['blanc', 'noir', 'vert']

# Afficher la matrice de confusion avec les étiquettes dans l'ordre spécifié
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=label_order, yticklabels=label_order)
plt.xlabel('Classe prédite')
plt.ylabel('Classe réelle')
plt.title('Matrice de confusion')
plt.show()

